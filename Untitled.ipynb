{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, input_dim, H, output_dim = 64, 1000, 100, 10\n",
    "\n",
    "x = Variable(torch.rand(N, input_dim))\n",
    "y = Variable(torch.rand(N, output_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.7559  0.6549  0.7759  ...   0.3799  0.8335  0.0190\n",
      " 0.1124  0.5866  0.4613  ...   0.6317  0.8931  0.6430\n",
      " 0.6143  0.7941  0.8672  ...   0.9098  0.7947  0.5198\n",
      "          ...             â‹±             ...          \n",
      " 0.9884  0.8736  0.9964  ...   0.6577  0.7261  0.7399\n",
      " 0.7336  0.3668  0.5410  ...   0.1621  0.5800  0.1574\n",
      " 0.1096  0.8561  0.7299  ...   0.9256  0.2577  0.8277\n",
      "[torch.FloatTensor of size 64x1000]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.3255  0.1011  0.5271  0.0204  0.5980  0.1314  0.6590  0.8440  0.8831  0.1702\n",
      " 0.5937  0.4552  0.0981  0.0268  0.4916  0.8990  0.6629  0.8481  0.2799  0.0595\n",
      " 0.9658  0.9305  0.5994  0.1814  0.0598  0.7888  0.7348  0.9132  0.4524  0.1747\n",
      " 0.3876  0.3259  0.5120  0.1158  0.0745  0.4475  0.6175  0.3228  0.1276  0.3884\n",
      " 0.7236  0.9552  0.5404  0.8077  0.3445  0.7471  0.5922  0.1202  0.5106  0.7435\n",
      " 0.4352  0.9642  0.1074  0.5267  0.4446  0.6151  0.4439  0.7580  0.7201  0.2174\n",
      " 0.1744  0.1474  0.8091  0.4056  0.6610  0.5247  0.6494  0.6754  0.7794  0.6641\n",
      " 0.1750  0.1217  0.6084  0.1461  0.2429  0.4453  0.1763  0.2100  0.0892  0.0844\n",
      " 0.4300  0.9306  0.6521  0.3702  0.5980  0.5016  0.5499  0.6740  0.1549  0.8128\n",
      " 0.2854  0.6958  0.2124  0.5702  0.5125  0.8257  0.1207  0.0156  0.4019  0.3075\n",
      " 0.6459  0.7392  0.3209  0.6281  0.0666  0.9708  0.3045  0.4465  0.1303  0.2457\n",
      " 0.3226  0.8626  0.2910  0.3091  0.8593  0.5404  0.7254  0.2549  0.7916  0.9871\n",
      " 0.9357  0.3885  0.5535  0.1674  0.1861  0.3916  0.0482  0.9105  0.4481  0.6597\n",
      " 0.4832  0.9785  0.1258  0.6524  0.9039  0.1640  0.7689  0.6193  0.9293  0.4268\n",
      " 0.1069  0.6636  0.1380  0.5251  0.6868  0.4431  0.9089  0.0118  0.9046  0.5834\n",
      " 0.7629  0.1473  0.1256  0.8601  0.3161  0.7321  0.2057  0.1798  0.7308  0.4640\n",
      " 0.1177  0.0188  0.7207  0.1012  0.4452  0.5808  0.1199  0.2535  0.4775  0.1839\n",
      " 0.8138  0.3719  0.9533  0.5117  0.5602  0.8506  0.7459  0.0619  0.6983  0.6540\n",
      " 0.2670  0.8161  0.2369  0.2598  0.2680  0.0801  0.4880  0.7788  0.5867  0.9760\n",
      " 0.1565  0.4147  0.9652  0.4222  0.0702  0.1219  0.9104  0.6456  0.8382  0.2932\n",
      " 0.7846  0.6788  0.0181  0.3138  0.8311  0.3386  0.8402  0.5524  0.3042  0.4012\n",
      " 0.6617  0.6379  0.8865  0.2916  0.0592  0.3545  0.0247  0.3440  0.2978  0.0109\n",
      " 0.9758  0.9239  0.9016  0.9515  0.9099  0.9791  0.3516  0.4285  0.3673  0.1674\n",
      " 0.3435  0.5966  0.1935  0.2764  0.6682  0.0790  0.6500  0.3201  0.5134  0.9072\n",
      " 0.1475  0.6910  0.6495  0.9692  0.2679  0.6184  0.2983  0.2241  0.4300  0.3115\n",
      " 0.6322  0.5467  0.8161  0.3660  0.9522  0.6166  0.5469  0.4461  0.3875  0.6149\n",
      " 0.0712  0.9334  0.6307  0.9808  0.2481  0.2947  0.3304  0.4382  0.0034  0.5688\n",
      " 0.3760  0.5604  0.2966  0.1504  0.2635  0.2368  0.7291  0.1443  0.9955  0.2258\n",
      " 0.9197  0.9069  0.9919  0.9222  0.0019  0.4162  0.7034  0.0869  0.3082  0.3074\n",
      " 0.1727  0.4547  0.8593  0.1373  0.2356  0.9361  0.2872  0.2216  0.9783  0.1785\n",
      " 0.6526  0.2548  0.2381  0.5658  0.7318  0.0063  0.7827  0.0410  0.4084  0.2666\n",
      " 0.2950  0.1137  0.2774  0.1875  0.2707  0.8701  0.8095  0.0793  0.0062  0.7425\n",
      " 0.6769  0.2082  0.1280  0.2640  0.7502  0.3589  0.0073  0.2795  0.3351  0.1404\n",
      " 0.0776  0.0265  0.6460  0.5592  0.4167  0.4287  0.3425  0.7218  0.8687  0.9395\n",
      " 0.7216  0.0355  0.9183  0.9863  0.8507  0.4861  0.6589  0.1538  0.0217  0.3578\n",
      " 0.2011  0.1472  0.8854  0.3640  0.5858  0.5666  0.3612  0.0391  0.4971  0.3760\n",
      " 0.3544  0.6566  0.4081  0.5367  0.2650  0.4451  0.4703  0.7674  0.1134  0.4632\n",
      " 0.4962  0.4915  0.2924  0.4238  0.1518  0.1084  0.5550  0.9618  0.0969  0.7993\n",
      " 0.2296  0.5338  0.1814  0.2071  0.5797  0.4787  0.0501  0.9320  0.4561  0.8297\n",
      " 0.6969  0.7471  0.5567  0.9265  0.3042  0.0520  0.7747  0.9571  0.9931  0.9548\n",
      " 0.7324  0.4462  0.5588  0.4695  0.1146  0.6367  0.8675  0.4376  0.4109  0.0649\n",
      " 0.1711  0.5095  0.3416  0.4458  0.6392  0.1964  0.8206  0.2761  0.2128  0.0992\n",
      " 0.3481  0.6489  0.8088  0.4715  0.5867  0.3828  0.4845  0.4534  0.7394  0.9631\n",
      " 0.9372  0.8841  0.0321  0.2546  0.2858  0.6176  0.4472  0.9786  0.6941  0.9317\n",
      " 0.6887  0.5209  0.3271  0.7224  0.9979  0.0550  0.1247  0.3675  0.2897  0.7132\n",
      " 0.9874  0.6422  0.1576  0.0352  0.8187  0.8126  0.6947  0.0480  0.6165  0.3613\n",
      " 0.6121  0.1472  0.2183  0.0740  0.9731  0.6854  0.2929  0.8099  0.9975  0.6976\n",
      " 0.9447  0.2603  0.9446  0.3755  0.9032  0.0317  0.2904  0.8666  0.8250  0.6871\n",
      " 0.5997  0.1851  0.9361  0.0936  0.7257  0.9365  0.3938  0.1362  0.5213  0.5406\n",
      " 0.7499  0.9289  0.0691  0.1762  0.6494  0.1039  0.9007  0.2309  0.7177  0.2138\n",
      " 0.1928  0.3231  0.5964  0.5584  0.1877  0.3082  0.2919  0.7117  0.6742  0.8791\n",
      " 0.4000  0.5702  0.7656  0.9868  0.3093  0.9379  0.7498  0.3032  0.6769  0.9486\n",
      " 0.7641  0.7536  0.0638  0.5816  0.3366  0.1176  0.1699  0.8781  0.7161  0.3079\n",
      " 0.0844  0.6382  0.2189  0.9728  0.5674  0.4457  0.9718  0.8556  0.6803  0.5125\n",
      " 0.6407  0.3174  0.0956  0.0298  0.5360  0.1425  0.4504  0.0959  0.2951  0.8145\n",
      " 0.4915  0.2947  0.0268  0.8715  0.7282  0.6506  0.4992  0.8143  0.0761  0.8713\n",
      " 0.8021  0.4481  0.0137  0.8213  0.3763  0.9775  0.8034  0.6225  0.0802  0.1845\n",
      " 0.1959  0.2155  0.7202  0.1871  0.0466  0.2164  0.5019  0.0749  0.6552  0.9553\n",
      " 0.1268  0.3198  0.1239  0.4292  0.9528  0.0139  0.6667  0.5684  0.6053  0.6711\n",
      " 0.4446  0.8796  0.8895  0.9023  0.1060  0.5360  0.8184  0.9076  0.9219  0.1186\n",
      " 0.9912  0.7783  0.7260  0.9544  0.1066  0.1258  0.5253  0.0806  0.3937  0.3942\n",
      " 0.0102  0.0880  0.4576  0.3673  0.4020  0.8217  0.5806  0.7422  0.1374  0.0690\n",
      " 0.7051  0.5733  0.3353  0.0164  0.4056  0.6520  0.2905  0.7221  0.1326  0.0793\n",
      " 0.7465  0.7664  0.6143  0.2199  0.2378  0.8824  0.2409  0.5046  0.4216  0.9077\n",
      "[torch.FloatTensor of size 64x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = torch.nn.Sequential(torch.nn.linear(input_dim, H),\n",
    "torch.nn.ReLU(),\n",
    "torch.nn.linear(H, output_dim),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(torch.nn.Linear(input_dim, H),\n",
    "torch.nn.ReLU(),\n",
    "torch.nn.Linear(H, output_dim),)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-4cf8b5c0f346>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-4cf8b5c0f346>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    optimizer = torch.optim.Adamax('params':model.parameters, learning_rate)\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "learning_rate = 1e-4\n",
    "for i in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    optimizer = torch.optim.Adamax('params':model.parameters, learning_rate)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
